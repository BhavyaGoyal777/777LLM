{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lessggo chapter 7\n"
     ]
    }
   ],
   "source": [
    "print(\"lessggo chapter 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "\n",
    "with open(file_path,'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of entries:\", len(data))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "  instruction_text = (\n",
    "      f\"Below is an instruction that describes a task . \"\n",
    "      f\"write a response that appropriately completes the request.\"\n",
    "      f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "\n",
    "  )\n",
    "  input_text = (\n",
    "      f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "  )\n",
    "  return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task . write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_respose = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_respose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "validation set length: 55\n",
      "test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)\n",
    "test_portion = int(len(data) * 0.1)\n",
    "val_portion = len(data)-train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion+test_portion]\n",
    "val_data = data[train_portion+test_portion:]\n",
    "\n",
    "print(\"Training set length:\",train_portion)\n",
    "print(\"validation set length:\",val_portion)\n",
    "print(\"test set length:\",test_portion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer):\n",
    "    self.data = data\n",
    "    self.tokenizer = tokenizer\n",
    "    self.encoded_texts = []\n",
    "\n",
    "    for entry in data:\n",
    "      instruction_plus_input = format_input(entry)\n",
    "      respose_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "      full_text = instruction_plus_input + respose_text\n",
    "\n",
    "      self.encoded_texts.append(\n",
    "          tokenizer.encode(full_text)\n",
    "      )\n",
    "  def __getitem__(self,index):\n",
    "    return self.encoded_texts[index]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)       \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Downloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, tiktoken\n",
      "Successfully installed regex-2024.11.6 tiktoken-0.9.0\n",
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "toknizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(toknizer.encode(\"<|endoftext|>\",allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft(\n",
    "    batch,\n",
    "    pad_token_id = 50256,\n",
    "    device='cpu'):\n",
    "  batch_max_length = max(len(item) +1 for item in batch)\n",
    "  inputs_lst, target_lst= [] ,[]\n",
    "\n",
    "\n",
    "  for item in batch:\n",
    "    new_item = item.copy()\n",
    "    print(\"1-->\",new_item)\n",
    "    new_item += [pad_token_id]\n",
    "    print(\"2-->\",new_item)\n",
    "    \n",
    "\n",
    "\n",
    "    padded = (\n",
    "        new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "    )\n",
    "    print(padded)\n",
    "\n",
    "    inputs = torch.tensor(padded[:-1])\n",
    "    print(\"3-->\",inputs)\n",
    "    targets = torch.tensor(padded[1:])\n",
    "    print(\"4-->\",targets)\n",
    "\n",
    "    inputs_lst.append(inputs)\n",
    "    target_lst.append(targets)\n",
    "\n",
    "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "  target_tensor = torch.stack(target_lst).to(device)\n",
    "\n",
    "  return inputs_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1--> [0, 1, 2, 3, 4]\n",
      "2--> [0, 1, 2, 3, 4, 50256]\n",
      "[0, 1, 2, 3, 4, 50256]\n",
      "3--> tensor([0, 1, 2, 3, 4])\n",
      "4--> tensor([    1,     2,     3,     4, 50256])\n",
      "1--> [5, 6]\n",
      "2--> [5, 6, 50256]\n",
      "[5, 6, 50256, 50256, 50256, 50256]\n",
      "3--> tensor([    5,     6, 50256, 50256, 50256])\n",
      "4--> tensor([    6, 50256, 50256, 50256, 50256])\n",
      "1--> [7, 8, 9]\n",
      "2--> [7, 8, 9, 50256]\n",
      "[7, 8, 9, 50256, 50256, 50256]\n",
      "3--> tensor([    7,     8,     9, 50256, 50256])\n",
      "4--> tensor([    8,     9, 50256, 50256, 50256])\n"
     ]
    }
   ],
   "source": [
    "input_tensors, target_tensors = custom_collate_draft(\n",
    "    batch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_func(\n",
    "    batch,\n",
    "    pad_token_id = 50256,\n",
    "    ignore_index = -100,\n",
    "    allowed_max_length = None,\n",
    "    device= 'cpu'):\n",
    "  \n",
    "\n",
    "  batch_max_length = max(len(item)+1 for item in batch)\n",
    "  inputs_lst , targets_lst=[],[]\n",
    "\n",
    "  for item in batch:\n",
    "    new_item = item.copy()\n",
    "    new_item += [pad_token_id]\n",
    "\n",
    "    padded = (\n",
    "        new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "    )\n",
    "\n",
    "    inputs = torch.tensor(padded[:-1])\n",
    "    targets = torch.tensor(padded[1:])\n",
    "\n",
    "    mask = targets == pad_token_id\n",
    "    # print(mask)\n",
    "\n",
    "    indices = torch.nonzero(mask).squeeze()\n",
    "    # print(indices)\n",
    "\n",
    "    if indices.numel() > 1:\n",
    "      targets[indices[1:]] = ignore_index\n",
    "\n",
    "    if allowed_max_length is not None:\n",
    "      inputs = inputs[:allowed_max_length]\n",
    "      targets = targets[:allowed_max_length] \n",
    "    inputs_lst.append(inputs)  \n",
    "    targets_lst.append(targets) \n",
    "\n",
    "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "  targets_tensor = torch.stack(targets_lst).to(device)\n",
    "  return inputs_tensor,targets_tensor \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,targets = custom_collate_func(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial \n",
    "\n",
    "customized_collate_func = partial(\n",
    "    custom_collate_func,\n",
    "    device = device ,\n",
    "    allowed_max_length = 1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(\n",
    "    train_data,\n",
    "    tokenizer=toknizer\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_func,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val_dataset = InstructionDataset(\n",
    "    val_data,\n",
    "    tokenizer=toknizer\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_func,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "test_dataset = InstructionDataset(\n",
    "    test_data,\n",
    "    tokenizer=toknizer\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "   test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_func,\n",
    "   shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loader:\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"train Loader:\")\n",
    "for inputs,targets in train_loader:\n",
    "  print(inputs.shape,targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2L/774M/checkpoint\n",
      "File already exists and is up-to-date: gpt2L/774M/encoder.json\n",
      "File already exists and is up-to-date: gpt2L/774M/hparams.json\n",
      "File already exists and is up-to-date: gpt2L/774M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2L/774M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2L/774M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2L/774M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1280)\n",
       "  (pos_emb): Embedding(1024, 1280)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (24): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (25): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (26): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (27): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (28): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (29): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (30): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (31): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (32): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (33): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (34): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (35): TransformerBlock(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (W_value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FFN(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): Gelu()\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from chap4 import GPTModel\n",
    "from chap5 import load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\":1024,\n",
    "    \"drop_rate\":0.0,\n",
    "    \"bias_\":True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "\"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "\"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "\"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "\"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-large (774M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size  = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "settings,params = download_and_load_gpt2(model_size,\n",
    "                                         models_dir=\"gpt2L\")\n",
    "\n",
    "model = GPTModel(\n",
    "    BASE_CONFIG\n",
    ")\n",
    "\n",
    "load_weights_into_gpt(model,params)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task . write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "He runs very fast.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "input_text = format_input(data[69])\n",
    "\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chap5 import generate , text_to_token_ids , token_ids_to_text\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(input_text,tokenizer=toknizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    "\n",
    "                    \n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids,tokenizer=toknizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Output:\n",
      "\n",
      "He runs very fast.\n",
      "\n",
      "### Instructions:\n",
      "\n",
      "Write a response that appropriately completes\n"
     ]
    }
   ],
   "source": [
    "response_text =  generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.9918\n",
      "Validation Loss: 3.9403\n"
     ]
    }
   ],
   "source": [
    "from chap5 import calc_loss_loader_data,train_model_simple\n",
    "\n",
    "model.to(device)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader_data(\n",
    "        model = model,\n",
    "        device = device,\n",
    "        data_loader=train_loader,\n",
    "        num_batches=5\n",
    "    )\n",
    "    val_loss = calc_loss_loader_data(\n",
    "        model = model,\n",
    "        device = device,\n",
    "        data_loader=val_loader,\n",
    "        num_batches=5\n",
    "    )\n",
    "\n",
    "print(f\"Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (step 000000): Train loss 2.343 | Val loss 2.348\n",
      "Ep 1 (step 000005): Train loss 0.902 | Val loss 0.897\n",
      "Ep 1 (step 000010): Train loss 0.779 | Val loss 0.814\n",
      "Ep 1 (step 000015): Train loss 0.703 | Val loss 0.807\n",
      "Ep 1 (step 000020): Train loss 0.696 | Val loss 0.747\n",
      "Ep 1 (step 000025): Train loss 0.691 | Val loss 0.810\n",
      "Ep 1 (step 000030): Train loss 0.646 | Val loss 0.742\n",
      "Ep 1 (step 000035): Train loss 0.708 | Val loss 0.769\n",
      "Ep 1 (step 000040): Train loss 0.600 | Val loss 0.733\n",
      "Ep 1 (step 000045): Train loss 0.688 | Val loss 0.661\n",
      "Ep 1 (step 000050): Train loss 0.581 | Val loss 0.682\n",
      "Ep 1 (step 000055): Train loss 0.527 | Val loss 0.665\n",
      "Ep 1 (step 000060): Train loss 0.545 | Val loss 0.679\n",
      "Ep 1 (step 000065): Train loss 0.573 | Val loss 0.662\n",
      "Ep 1 (step 000070): Train loss 0.456 | Val loss 0.653\n",
      "Ep 1 (step 000075): Train loss 0.491 | Val loss 0.669\n",
      "Ep 1 (step 000080): Train loss 0.547 | Val loss 0.665\n",
      "Ep 1 (step 000085): Train loss 0.483 | Val loss 0.657\n",
      "Ep 1 (step 000090): Train loss 0.437 | Val loss 0.596\n",
      "Ep 1 (step 000095): Train loss 0.527 | Val loss 0.666\n",
      "Ep 1 (step 000100): Train loss 0.437 | Val loss 0.598\n",
      "Ep 1 (step 000105): Train loss 0.357 | Val loss 0.563\n",
      "Ep 1 (step 000110): Train loss 0.448 | Val loss 0.641\n",
      "Ep 1 (step 000115): Train loss 0.360 | Val loss 0.578\n",
      "Below is an instruction that describes a task . write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: Every day the chef cooks a meal of deliciousness.<|endoftext|>Role of solar radiation  In remote areas where lighting is scarce, living on the radioactive Moon could be uncomfortably cozy, as it has been found that\n",
      "Ep 2 (step 000120): Train loss 0.404 | Val loss 0.587\n",
      "Ep 2 (step 000125): Train loss 0.370 | Val loss 0.625\n",
      "Ep 2 (step 000130): Train loss 0.336 | Val loss 0.625\n",
      "Ep 2 (step 000135): Train loss 0.362 | Val loss 0.589\n",
      "Ep 2 (step 000140): Train loss 0.353 | Val loss 0.663\n",
      "Ep 2 (step 000145): Train loss 0.334 | Val loss 0.684\n",
      "Ep 2 (step 000150): Train loss 0.301 | Val loss 0.619\n",
      "Ep 2 (step 000155): Train loss 0.346 | Val loss 0.615\n",
      "Ep 2 (step 000160): Train loss 0.376 | Val loss 0.606\n",
      "Ep 2 (step 000165): Train loss 0.294 | Val loss 0.629\n",
      "Ep 2 (step 000170): Train loss 0.327 | Val loss 0.644\n",
      "Ep 2 (step 000175): Train loss 0.344 | Val loss 0.595\n",
      "Ep 2 (step 000180): Train loss 0.317 | Val loss 0.628\n",
      "Ep 2 (step 000185): Train loss 0.305 | Val loss 0.616\n",
      "Ep 2 (step 000190): Train loss 0.315 | Val loss 0.595\n",
      "Ep 2 (step 000195): Train loss 0.343 | Val loss 0.582\n",
      "Ep 2 (step 000200): Train loss 0.311 | Val loss 0.550\n",
      "Ep 2 (step 000205): Train loss 0.288 | Val loss 0.602\n",
      "Ep 2 (step 000210): Train loss 0.289 | Val loss 0.590\n",
      "Ep 2 (step 000215): Train loss 0.275 | Val loss 0.601\n",
      "Ep 2 (step 000220): Train loss 0.253 | Val loss 0.577\n",
      "Ep 2 (step 000225): Train loss 0.239 | Val loss 0.571\n",
      "Ep 2 (step 000230): Train loss 0.297 | Val loss 0.606\n",
      "Below is an instruction that describes a task . write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Input: The chef cooks the meal every day.  ### Response: The meal is cooked by the chef.<|endoftext|>Find out how much pockets can store.  How much can pockets store?  steel chain \n",
      "Training completed in 3.74 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(\n",
    "model.parameters(), lr=0.00005, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "model, train_loader, val_loader, optimizer, device,\n",
    "num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "start_context=format_input(val_data[0]), tokenizer=toknizer\n",
    ")\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chap5 import plot_losses\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses,num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task . write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\\n>> Jane Austen.\n",
      "\n",
      "Model response:\\n>> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "for entry in test_data[:3]: #1\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate( #2\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, toknizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    "    )\n",
    "generated_text = token_ids_to_text(token_ids, toknizer)\n",
    "response_text = (\n",
    "generated_text[len(input_text):]\n",
    ".replace(\"### Response:\", \"\")\n",
    ".strip()\n",
    ")\n",
    "print(input_text)\n",
    "print(f\"\\nCorrect response:\\\\n>> {entry['output']}\")\n",
    "print(f\"\\nModel response:\\\\n>> {response_text.strip()}\")\n",
    "print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [01:10<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, toknizer).to(device),\n",
    "    max_new_tokens=256,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, toknizer)\n",
    "    response_text = (\n",
    "    generated_text[len(input_text):]\n",
    ".replace(\"### Response:\", \"\")\n",
    ".strip()\n",
    ")\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a cheetah.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as gpt2-large774M-sft.pth\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\" #1\n",
    "torch.save(model.state_dict(), file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"gpt2-large774M-sft.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
